{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# directory path of everything \n",
    "!ls /kaggle/input/llama-3.1/transformers/8b-instruct/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM,pipeline\n",
    "import torch\n",
    "\n",
    "base_model = \"/kaggle/input/llama-3.1/transformers/8b-instruct/1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check if gpu is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"What is the tallest building in the world?\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=120, do_sample=True)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-25T19:50:55.393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -U bitsandbytes\n",
    "%pip install -U transformers\n",
    "%pip install -U accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:11:29.198203Z",
     "iopub.status.busy": "2025-02-25T20:11:29.197911Z",
     "iopub.status.idle": "2025-02-25T20:11:29.365190Z",
     "shell.execute_reply": "2025-02-25T20:11:29.364355Z",
     "shell.execute_reply.started": "2025-02-25T20:11:29.198180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "wb_token = user_secrets.get_secret(\"wandb\")\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune llama-3.1-8b-it on Sentiment Analysis Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:51:26.416355Z",
     "iopub.status.busy": "2025-02-25T19:51:26.416024Z",
     "iopub.status.idle": "2025-02-25T19:51:26.422510Z",
     "shell.execute_reply": "2025-02-25T19:51:26.421565Z",
     "shell.execute_reply.started": "2025-02-25T19:51:26.416326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Write df to disk to save RAM storage due to training issues \n",
    "input_path = \"/kaggle/input/sentiment-analysis-for-mental-health/Combined Data.csv\"\n",
    "output_path = \"/kaggle/working/Filtered_Data.parquet\"\n",
    "\n",
    "\n",
    "chunksize = 50000  \n",
    "\n",
    "first_chunk = not os.path.exists(output_path)  \n",
    "\n",
    "\n",
    "for chunk in pd.read_csv(input_path, index_col=\"Unnamed: 0\", chunksize=chunksize):\n",
    "    \n",
    "    chunk.loc[:, \"status\"] = chunk[\"status\"].str.replace(\"Bi-Polar\", \"Bipolar\")\n",
    "    chunk = chunk[~chunk[\"status\"].isin([\"Personality disorder\", \"Stress\", \"Suicidal\"])]\n",
    "\n",
    "    \n",
    "    if first_chunk:\n",
    "        chunk.to_parquet(output_path, compression=\"gzip\", index=False, engine=\"pyarrow\")\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        \n",
    "        existing_df = pd.read_parquet(output_path, engine=\"pyarrow\")\n",
    "        combined_df = pd.concat([existing_df, chunk], ignore_index=True)\n",
    "        combined_df.to_parquet(output_path, compression=\"gzip\", index=False, engine=\"pyarrow\")\n",
    "\n",
    "    \n",
    "    del chunk\n",
    "    gc.collect()  \n",
    "\n",
    "print(\"df loaded to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:51:32.173490Z",
     "iopub.status.busy": "2025-02-25T19:51:32.173161Z",
     "iopub.status.idle": "2025-02-25T19:51:32.378192Z",
     "shell.execute_reply": "2025-02-25T19:51:32.377172Z",
     "shell.execute_reply.started": "2025-02-25T19:51:32.173460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df is now on disk instead of RAM\n",
    "output_path = \"/kaggle/working/Filtered_Data.parquet\"\n",
    "\n",
    "df = pd.read_parquet(output_path, engine=\"pyarrow\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:51:35.482372Z",
     "iopub.status.busy": "2025-02-25T19:51:35.482051Z",
     "iopub.status.idle": "2025-02-25T19:51:35.695134Z",
     "shell.execute_reply": "2025-02-25T19:51:35.694163Z",
     "shell.execute_reply.started": "2025-02-25T19:51:35.482344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# we should show emphasize empathetic care when...\n",
    "def map_to_empathy_label(row):\n",
    "    if row['status'] in ['Depression', 'Anxiety', 'Bipolar']:\n",
    "        return 'Empathy'  \n",
    "    else:\n",
    "        return 'No Empathy'\n",
    "\n",
    "df['empathy_label'] = df.apply(map_to_empathy_label, axis=1)\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Classify the text into Empathy or No Empathy, and return the answer as the corresponding label.\n",
    "text: {data_point[\"statement\"]}\n",
    "label: {data_point[\"empathy_label\"]}\"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "     return f\"\"\"\n",
    "            Classify the text into Empathy or No Empathy, and return the answer as the corresponding label.\n",
    "text: {data_point[\"statement\"]}\n",
    "label: \"\"\".strip()\n",
    "\n",
    "\n",
    "df = df.sample(frac=1, random_state=85).reset_index(drop=True).head(1500)\n",
    "\n",
    "train_size = 0.8\n",
    "eval_size = 0.1\n",
    "\n",
    "train_end = int(train_size * len(df))\n",
    "eval_end = train_end + int(eval_size * len(df))\n",
    "\n",
    "X_train = df[:train_end]\n",
    "X_eval = df[train_end:eval_end]\n",
    "X_test = df[eval_end:]\n",
    "\n",
    "\n",
    "X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n",
    "X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n",
    "y_true = X_test.loc[:,'empathy_label']  \n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "\n",
    "\n",
    "X_train['empathy_label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:51:38.207911Z",
     "iopub.status.busy": "2025-02-25T19:51:38.207562Z",
     "iopub.status.idle": "2025-02-25T19:51:38.227246Z",
     "shell.execute_reply": "2025-02-25T19:51:38.226107Z",
     "shell.execute_reply.started": "2025-02-25T19:51:38.207883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# convert train and eval to datasets\n",
    "train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
    "eval_data = Dataset.from_pandas(X_eval[[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:51:41.907384Z",
     "iopub.status.busy": "2025-02-25T19:51:41.907075Z",
     "iopub.status.idle": "2025-02-25T19:52:01.523292Z",
     "shell.execute_reply": "2025-02-25T19:52:01.522608Z",
     "shell.execute_reply.started": "2025-02-25T19:51:41.907358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_model_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:54:02.371737Z",
     "iopub.status.busy": "2025-02-25T19:54:02.371372Z",
     "iopub.status.idle": "2025-02-25T19:54:53.647321Z",
     "shell.execute_reply": "2025-02-25T19:54:53.646263Z",
     "shell.execute_reply.started": "2025-02-25T19:54:02.371707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    categories = [\"Empathy\", \"No Empathy\"]  \n",
    "    \n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"text\"]\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens=2, \n",
    "                        temperature=0.1)\n",
    "        \n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"label:\")[-1].strip()\n",
    "        \n",
    "        \n",
    "        for category in categories:\n",
    "            if category.lower() in answer.lower():\n",
    "                y_pred.append(category)\n",
    "                break\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(X_test, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:54:59.390081Z",
     "iopub.status.busy": "2025-02-25T19:54:59.389793Z",
     "iopub.status.idle": "2025-02-25T19:54:59.412385Z",
     "shell.execute_reply": "2025-02-25T19:54:59.411389Z",
     "shell.execute_reply.started": "2025-02-25T19:54:59.390060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    labels = [\"Empathy\", \"No Empathy\"]\n",
    "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "    \n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)  \n",
    "    \n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    \n",
    "    unique_labels = set(y_true_mapped)  \n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')\n",
    "        \n",
    "    \n",
    "    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(labels))))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n",
    "evaluate(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:55:07.015338Z",
     "iopub.status.busy": "2025-02-25T19:55:07.015034Z",
     "iopub.status.idle": "2025-02-25T19:55:07.023046Z",
     "shell.execute_reply": "2025-02-25T19:55:07.022175Z",
     "shell.execute_reply.started": "2025-02-25T19:55:07.015304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  \n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pre-process data \n",
    "\n",
    "# to satisfy max_seq_length req for trainer\n",
    "def preprocess_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "train_data = train_data.map(preprocess_function)\n",
    "eval_data = eval_data.map(preprocess_function)\n",
    "\n",
    "train_data = train_data.shuffle(seed=42).select(range(int(len(train_data) * 0.5)))\n",
    "eval_data = eval_data.shuffle(seed=42).select(range(int(len(eval_data) * 0.5)))\n",
    "\n",
    "\n",
    "X_train.to_parquet(\"/kaggle/working/X_train.parquet\", compression='gzip')\n",
    "X_eval.to_parquet(\"/kaggle/working/X_eval.parquet\", compression='gzip')\n",
    "print(\"x_train and x_eval written to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:55:49.003063Z",
     "iopub.status.busy": "2025-02-25T19:55:49.002747Z",
     "iopub.status.idle": "2025-02-25T19:55:52.979430Z",
     "shell.execute_reply": "2025-02-25T19:55:52.978572Z",
     "shell.execute_reply.started": "2025-02-25T19:55:49.003041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir=\"llama-3.1-fine-tuned-model\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules,\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                    \n",
    "    num_train_epochs=1,                       \n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,            \n",
    "    gradient_checkpointing=True,              \n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=1,                         \n",
    "    learning_rate=2e-4,                       \n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        \n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        \n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",               \n",
    "    report_to=\"wandb\",                  \n",
    "    eval_strategy=\"steps\",              \n",
    "    eval_steps = 0.2\n",
    ")\n",
    "\n",
    "# adjusted for build\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:57:31.155552Z",
     "iopub.status.busy": "2025-02-25T19:57:31.155175Z",
     "iopub.status.idle": "2025-02-25T19:57:31.480341Z",
     "shell.execute_reply": "2025-02-25T19:57:31.479673Z",
     "shell.execute_reply.started": "2025-02-25T19:57:31.155520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# need to free up memory in cuda \n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# reduce fragmentation to help out with RAM\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T19:57:43.602997Z",
     "iopub.status.busy": "2025-02-25T19:57:43.602681Z",
     "iopub.status.idle": "2025-02-25T20:10:40.475719Z",
     "shell.execute_reply": "2025-02-25T20:10:40.474820Z",
     "shell.execute_reply.started": "2025-02-25T19:57:43.602972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# list all variables that use memory (particularly those that take up most space)\n",
    "vars_in_mem = {k: sys.getsizeof(v) for k, v in globals().items()}\n",
    "sorted_vars = sorted(vars_in_mem.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pd.DataFrame(sorted_vars[:10], columns=[\"Variable\", \"Size (bytes)\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:11:34.460699Z",
     "iopub.status.busy": "2025-02-25T20:11:34.460372Z",
     "iopub.status.idle": "2025-02-25T20:11:35.944086Z",
     "shell.execute_reply": "2025-02-25T20:11:35.943423Z",
     "shell.execute_reply.started": "2025-02-25T20:11:34.460673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# finish weights and biases run\n",
    "wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:14:07.096842Z",
     "iopub.status.busy": "2025-02-25T20:14:07.096451Z",
     "iopub.status.idle": "2025-02-25T20:14:08.924966Z",
     "shell.execute_reply": "2025-02-25T20:14:08.924199Z",
     "shell.execute_reply.started": "2025-02-25T20:14:07.096807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"fintuned model and tokenizer written to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:14:47.159409Z",
     "iopub.status.busy": "2025-02-25T20:14:47.159105Z",
     "iopub.status.idle": "2025-02-25T20:15:59.047730Z",
     "shell.execute_reply": "2025-02-25T20:15:59.046816Z",
     "shell.execute_reply.started": "2025-02-25T20:14:47.159386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5338273,
     "sourceId": 8870083,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 81881,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
